{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Notebook\n",
    "This notebook prepares a flat dataset of GPS tracking points for further analysis.\n",
    "* Data is loaded from .gpx files to a Spark SQL `DataFrame` using the custom `StravaLoader` class\n",
    "* Nested columns are flattened \n",
    "* Additional attributes such as speed and accumulated distance are calculated\n",
    "* Tracking points where the athlete is at rest are filtered out to better calculate pause times\n",
    "\n",
    "## Input parameters\n",
    "- `dataset` - The name of the dataset to load. Use \"strava-activities\" or \"strava-activities-subset\"\n",
    "- `pause_threshold_minutes` - The minimum gap in minutes between two *activity_blocks*. If the time gap between to consecutive tracking points is greater than this limit the two points belong to different blocks\n",
    "- `rest_speed_threshold_kmh[acticity_type]` - The upper limit for when to consider the athlete to be *at rest* for each `activity_type`. If the speed is lower than this limit the athlete is at rest and the tracking point will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'strava-activities-subset'\n",
    "pause_threshold_minutes = 10\n",
    "rest_speed_threshold_kmh = {\n",
    "    'Ride': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import coalesce, concat, lag, lit, lower, sum, unix_timestamp, max, min, \\\n",
    "                                  sin, cos, atan, sqrt, atan2, toRadians, round, avg, first\n",
    "from classes import StravaLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data with `StravaLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Using supplied SparkContext and HiveContext\n",
      "Warning: Automatic directory/athlete detection not yet supported for data source s3. Using: \"akrogvig\", \"lkrogvig\", \"brustad\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize Strava activity loader\n",
    "sl = StravaLoader('s3', dataset, sc=sc, hiveContext=sqlContext)\n",
    "\n",
    "# Load the dataset\n",
    "df = sl.get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking point dataset\n",
    "Compute time gap between consecutive points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Partitioning on <athlete> and <activity_type>\n",
    "window = Window.partitionBy('athlete', 'activity_type').orderBy('TIME_unix_time')\n",
    "\n",
    "# Timestamp in seconds\n",
    "df = df.withColumn( \n",
    "    'TIME_unix_time', \n",
    "    unix_timestamp(df['time'], \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for calulation of speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_speed(df, window):\n",
    "    \n",
    "    R = 6371000 # Earth radius\n",
    "    \n",
    "    # Time difference in seconds between tracking point and previous tracking point\n",
    "    df = df.withColumn(\n",
    "        'TIME_unix_time_diff',\n",
    "        df['TIME_unix_time'] - lag('TIME_unix_time', count=1).over(window)\n",
    "    )\n",
    "\n",
    "    # Latitude and longditude in radians\n",
    "    df = df.withColumn(\n",
    "        'DIST_@latR',\n",
    "        toRadians(df['@lat'])\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        'DIST_@lonR',\n",
    "        toRadians(df['@lon'])\n",
    "    )\n",
    "    \n",
    "    # Latitude and longditude in previous tracking point\n",
    "    df = df.withColumn(\n",
    "        'DIST_@latR_prev',\n",
    "        lag('DIST_@latR', count=1).over(window)\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        'DIST_@lonR_prev',\n",
    "        lag('DIST_@lonR', count=1).over(window)\n",
    "    )\n",
    "    \n",
    "    # Difference in latitude and longditude since previous tracking point\n",
    "    df = df.withColumn(\n",
    "        'DIST_@latR_diff',\n",
    "        coalesce(df['DIST_@latR'] - df['DIST_@latR_prev'], lit(0))\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        'DIST_@lonR_diff',\n",
    "        coalesce(df['DIST_@lonR'] - df['DIST_@lonR_prev'], lit(0))\n",
    "    )\n",
    "\n",
    "    # Havesine distance calculation between two tracking points\n",
    "    df = df.withColumn(\n",
    "        'DIST_a',\n",
    "        sin(df['DIST_@latR_diff']/2) * sin(df['DIST_@latR_diff']/2)\n",
    "        + cos(df['DIST_@latR']) * cos(df['DIST_@latR_prev'])\n",
    "        * sin(df['DIST_@lonR_diff']/2) * sin(df['DIST_@lonR_diff']/2)\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        'DIST_c',\n",
    "        2 * atan2(sqrt(df['DIST_a']), sqrt(1 - df['DIST_a']))\n",
    "    )\n",
    "    \n",
    "    # Distance between consecutive points\n",
    "    df = df.withColumn(\n",
    "        'DIST_diff_meters',\n",
    "        coalesce(R * df['DIST_c'], lit(0))\n",
    "    )\n",
    "    \n",
    "    # Momentary speed in km/h\n",
    "    df = df.withColumn(\n",
    "        'SPEED_kmh',\n",
    "        coalesce(\n",
    "            3.6 * df['DIST_diff_meters'] / (df['TIME_unix_time'] - lag('TIME_unix_time', count=1).over(window)),\n",
    "            lit(0)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate speed, filter points at rest. Remember to recalculate speed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = calculate_speed(df, window)\n",
    "df = df.filter((df['activity_type']=='Ride') & (df['SPEED_kmh']>rest_speed_threshold_kmh['Ride']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive activity blocks by checking distance between consecutive tracking points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indicator (0,1) of whether time difference is greater than threshold (new activity block)\n",
    "df = df.withColumn(\n",
    "    'BLOCK_isnew',\n",
    "    coalesce(\n",
    "        (df['TIME_unix_time_diff'] >= pause_threshold_minutes * 60).cast('integer'),\n",
    "        lit(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sequence number of activity block per athlete and activity \n",
    "df = df.withColumn(\n",
    "    'BLOCK_seqnum', \n",
    "    sum('BLOCK_isnew').over(window)\n",
    ")\n",
    "\n",
    "# Activity block id \"<athlete>_<activity_type>_<integer>\"\n",
    "df = df.withColumn(\n",
    "    'BLOCK_id',\n",
    "    concat(\n",
    "        df['athlete'],\n",
    "        lit('_'),\n",
    "        lower(df['activity_type']),\n",
    "        lit('_'),\n",
    "        df['BLOCK_seqnum'].cast('string')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculate speed, and compute activity block specific metrics, like accumulated distance within each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_block = Window.partitionBy('BLOCK_id').orderBy('TIME_unix_time')\n",
    "\n",
    "# Recalculate speed within each block\n",
    "df = calculate_speed(df, window_block)\n",
    "\n",
    "# Current total distance in km within the block\n",
    "df = df.withColumn(\n",
    "    'DIST_BLOCK_km',\n",
    "    sum('DIST_diff_meters').over(window_block) / 1000\n",
    ")\n",
    "\n",
    "# Current time elapsed in seconds within the block\n",
    "df = df.withColumn(\n",
    "    'TIME_BLOCK_elapsed_seconds',\n",
    "    df['TIME_unix_time'] - first(df['TIME_unix_time']).over(window_block)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, flatten the `DataFrame` and remove redundant columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dff = df.select(\n",
    "    df['athlete'].alias('athlete'), \n",
    "    df['activity_type'].alias('activity_type'),\n",
    "    df['BLOCK_id'].alias('block_id'),\n",
    "    df['time'].alias('p_timestamp'),\n",
    "    df['@lat'].alias('p_latitude'), \n",
    "    df['@lon'].alias('p_longditude'), \n",
    "    df['ele'].alias('p_elevation'),\n",
    "    df['SPEED_kmh'].alias('p_speed_kmh'),\n",
    "    df['extensions.gpxtpx:TrackPointExtension.gpxtpx:atemp'].alias('p_temperature'), \n",
    "    df['extensions.gpxtpx:TrackPointExtension.gpxtpx:cad'].alias('p_cadence'), \n",
    "    df['extensions.gpxtpx:TrackPointExtension.gpxtpx:hr'].alias('p_heart_rate'), \n",
    "    df['DIST_diff_meters'].alias('p_diff_distance_meters'),\n",
    "    df['TIME_unix_time_diff'].alias('p_diff_time_seconds'),\n",
    "    df['DIST_BLOCK_km'].alias('c_cumlative_distance_km'),\n",
    "    df['TIME_BLOCK_elapsed_seconds'].alias('c_elapsed_seconds')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save flat dataset in parquet file format for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dff.write.mode('overwrite').parquet('s3n://larsbk/parquet/%s/' % dataset) # sl.path -> sl.root_path, sl.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity block level aggregation dataset\n",
    "\n",
    "Compute columns to use in calculations of weighted averages, like average heart rate $h_{avg}$\n",
    "\n",
    "\\begin{equation}\n",
    "h_{avg} = \\frac{1}{\\sum_i{\\Delta t_i}}\\sum_{i}{\\Delta t_i \\cdot \\frac{h_{i+1}+h_i}{2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get heart rate from extensions struct to use lag function\n",
    "df = df.withColumn(\n",
    "    'HEART_RATE',\n",
    "    df['extensions.gpxtpx:TrackPointExtension.gpxtpx:hr']\n",
    ")\n",
    "\n",
    "# Average heart rate of two consecutive tracking points\n",
    "df = df.withColumn(\n",
    "    'MIDPOINT_heart_rate',\n",
    "    0.5 * (df['HEART_RATE']\n",
    "    + lag(df['HEART_RATE'], count=1).over(window_block))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all aggregations simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa = df.groupBy('block_id').agg(\n",
    "    min(df['time']).alias('block_start_time'),\n",
    "    max(df['DIST_BLOCK_km']).alias('block_total_distance_km'),\n",
    "    (max(df['TIME_BLOCK_elapsed_seconds'])/60).alias('block_duration_minutes'),\n",
    "    (max(df['DIST_BLOCK_km']) / (max(df['TIME_BLOCK_elapsed_seconds']) / 3600)).alias('block_avg_speed_kmh'),\n",
    "    sum(df['TIME_unix_time_diff'] * df['MIDPOINT_heart_rate']) / max(df['TIME_unix_time_diff']).alias('block_avg_heart_rate') \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save aggregated dataset for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfa.write.mode('overwrite').parquet('s3n://larsbk/parquet/agg/%s/' % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         p_timestamp|p_diff_time_seconds|\n",
      "+--------------------+-------------------+\n",
      "|2014-06-03T15:39:26Z|               null|\n",
      "|2014-06-03T15:39:27Z|                  1|\n",
      "|2014-02-08T15:27:10Z|               null|\n",
      "|2014-02-08T15:18:59Z|               null|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.filter('c_elapsed_seconds<2').select(['p_timestamp', 'p_diff_time_seconds']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
